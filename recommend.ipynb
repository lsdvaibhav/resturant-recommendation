{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zomato Restaurant Recommendation System with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will introduce you to a machine learning project on Zomato Restaurant Recommendation System with Python programming language. There is an extended class of applications that involve predicting user responses to a variety of options. Such a system is called a recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Restaurant recommendation system uses content-based filtering. This method only uses information about the description and attributes of items that users have previously consumed to model user preferences.\n",
    "\n",
    "In other words, these algorithms try to recommend things similar to what a user liked in the past. The dataset Iâ€™ll be using here consists of restaurants in Bangalore, India, collected from Zomato. You can download the dataset from https://www.kaggle.com/himanshupoddar/zomato-bangalore-restaurants/download.\n",
    "\n",
    "To create the Restaurant recommendation system, I will create a content-based recommendation system where when I enter the name of a restaurant, the Restaurant recommendation system will look at reviews from other restaurants, and System will recommend us to the other restaurants with similar reviews and sort them from the top-rated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will start the task of Zomato Restaurant Recommendation System by installing the necessary Python Libraries using pip:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.11.1-py3-none-any.whl (285 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from seaborn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from seaborn) (1.5.4)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from seaborn) (1.1.4)\n",
      "Collecting matplotlib>=2.2\n",
      "  Using cached matplotlib-3.3.3-cp37-cp37m-win_amd64.whl (8.5 MB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.3.1-cp37-cp37m-win_amd64.whl (51 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from pandas>=0.23->seaborn) (2020.4)\n",
      "Installing collected packages: kiwisolver, cycler, matplotlib, seaborn\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.3 seaborn-0.11.1\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.0-cp37-cp37m-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=28d46a16576ccd4a9420431934dd7da3693a8db5fdb4565c065185516d3a603e\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\46\\ef\\c3\\157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.0 scikit-learn-0.24.0 sklearn-0.0 threadpoolctl-2.1.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\desktop\\vaibhav\\env\\lib\\site-packages (from nltk) (1.0.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.55.1-py2.py3-none-any.whl (68 kB)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434673 sha256=c2db53285113df0f267178c8136d01ef488bb922ed50b8d01d3bec583a111515\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\45\\6c\\46\\a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.5 regex-2020.11.13 tqdm-4.55.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re\n",
      "ERROR: No matching distribution found for re\n",
      "ERROR: Could not find a version that satisfies the requirement warnings\n",
      "ERROR: No matching distribution found for warnings\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now ,I will start  importing the necessary Python Libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, I will load and read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zomato.com/bangalore/jalsa-banasha...</td>\n",
       "      <td>942, 21st Main Road, 2nd Stage, Banashankari, ...</td>\n",
       "      <td>Jalsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>775</td>\n",
       "      <td>080 42297555\\r\\n+91 9743772233</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Pasta, Lunch Buffet, Masala Papad, Paneer Laja...</td>\n",
       "      <td>North Indian, Mughlai, Chinese</td>\n",
       "      <td>800</td>\n",
       "      <td>[('Rated 4.0', 'RATED\\n  A beautiful place to ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Buffet</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.zomato.com/bangalore/spice-elephan...</td>\n",
       "      <td>2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...</td>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>787</td>\n",
       "      <td>080 41714161</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Momos, Lunch Buffet, Chocolate Nirvana, Thai G...</td>\n",
       "      <td>Chinese, North Indian, Thai</td>\n",
       "      <td>800</td>\n",
       "      <td>[('Rated 4.0', 'RATED\\n  Had been here for din...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Buffet</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.zomato.com/SanchurroBangalore?cont...</td>\n",
       "      <td>1112, Next to KIMS Medical College, 17th Cross...</td>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3.8/5</td>\n",
       "      <td>918</td>\n",
       "      <td>+91 9663487993</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Cafe, Casual Dining</td>\n",
       "      <td>Churros, Cannelloni, Minestrone Soup, Hot Choc...</td>\n",
       "      <td>Cafe, Mexican, Italian</td>\n",
       "      <td>800</td>\n",
       "      <td>[('Rated 3.0', \"RATED\\n  Ambience is not that ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Buffet</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.zomato.com/bangalore/addhuri-udupi...</td>\n",
       "      <td>1st Floor, Annakuteera, 3rd Stage, Banashankar...</td>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3.7/5</td>\n",
       "      <td>88</td>\n",
       "      <td>+91 9620009302</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Quick Bites</td>\n",
       "      <td>Masala Dosa</td>\n",
       "      <td>South Indian, North Indian</td>\n",
       "      <td>300</td>\n",
       "      <td>[('Rated 4.0', \"RATED\\n  Great food and proper...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Buffet</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.zomato.com/bangalore/grand-village...</td>\n",
       "      <td>10, 3rd Floor, Lakshmi Associates, Gandhi Baza...</td>\n",
       "      <td>Grand Village</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3.8/5</td>\n",
       "      <td>166</td>\n",
       "      <td>+91 8026612447\\r\\n+91 9901210005</td>\n",
       "      <td>Basavanagudi</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Panipuri, Gol Gappe</td>\n",
       "      <td>North Indian, Rajasthani</td>\n",
       "      <td>600</td>\n",
       "      <td>[('Rated 4.0', 'RATED\\n  Very good restaurant ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Buffet</td>\n",
       "      <td>Banashankari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.zomato.com/bangalore/jalsa-banasha...   \n",
       "1  https://www.zomato.com/bangalore/spice-elephan...   \n",
       "2  https://www.zomato.com/SanchurroBangalore?cont...   \n",
       "3  https://www.zomato.com/bangalore/addhuri-udupi...   \n",
       "4  https://www.zomato.com/bangalore/grand-village...   \n",
       "\n",
       "                                             address                   name  \\\n",
       "0  942, 21st Main Road, 2nd Stage, Banashankari, ...                  Jalsa   \n",
       "1  2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...         Spice Elephant   \n",
       "2  1112, Next to KIMS Medical College, 17th Cross...        San Churro Cafe   \n",
       "3  1st Floor, Annakuteera, 3rd Stage, Banashankar...  Addhuri Udupi Bhojana   \n",
       "4  10, 3rd Floor, Lakshmi Associates, Gandhi Baza...          Grand Village   \n",
       "\n",
       "  online_order book_table   rate  votes                             phone  \\\n",
       "0          Yes        Yes  4.1/5    775    080 42297555\\r\\n+91 9743772233   \n",
       "1          Yes         No  4.1/5    787                      080 41714161   \n",
       "2          Yes         No  3.8/5    918                    +91 9663487993   \n",
       "3           No         No  3.7/5     88                    +91 9620009302   \n",
       "4           No         No  3.8/5    166  +91 8026612447\\r\\n+91 9901210005   \n",
       "\n",
       "       location            rest_type  \\\n",
       "0  Banashankari        Casual Dining   \n",
       "1  Banashankari        Casual Dining   \n",
       "2  Banashankari  Cafe, Casual Dining   \n",
       "3  Banashankari          Quick Bites   \n",
       "4  Basavanagudi        Casual Dining   \n",
       "\n",
       "                                          dish_liked  \\\n",
       "0  Pasta, Lunch Buffet, Masala Papad, Paneer Laja...   \n",
       "1  Momos, Lunch Buffet, Chocolate Nirvana, Thai G...   \n",
       "2  Churros, Cannelloni, Minestrone Soup, Hot Choc...   \n",
       "3                                        Masala Dosa   \n",
       "4                                Panipuri, Gol Gappe   \n",
       "\n",
       "                         cuisines approx_cost(for two people)  \\\n",
       "0  North Indian, Mughlai, Chinese                         800   \n",
       "1     Chinese, North Indian, Thai                         800   \n",
       "2          Cafe, Mexican, Italian                         800   \n",
       "3      South Indian, North Indian                         300   \n",
       "4        North Indian, Rajasthani                         600   \n",
       "\n",
       "                                        reviews_list menu_item  \\\n",
       "0  [('Rated 4.0', 'RATED\\n  A beautiful place to ...        []   \n",
       "1  [('Rated 4.0', 'RATED\\n  Had been here for din...        []   \n",
       "2  [('Rated 3.0', \"RATED\\n  Ambience is not that ...        []   \n",
       "3  [('Rated 4.0', \"RATED\\n  Great food and proper...        []   \n",
       "4  [('Rated 4.0', 'RATED\\n  Very good restaurant ...        []   \n",
       "\n",
       "  listed_in(type) listed_in(city)  \n",
       "0          Buffet    Banashankari  \n",
       "1          Buffet    Banashankari  \n",
       "2          Buffet    Banashankari  \n",
       "3          Buffet    Banashankari  \n",
       "4          Buffet    Banashankari  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zomato_real = pd.read_csv(\"zomato.csv\")\n",
    "\n",
    "# prints the first 5 rows of the dataset\n",
    "zomato_real.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the next step is data cleaning and feature engineering for this step we need to do a lot of stuff with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting Unnnecessary Columns\n",
    "zomato=zomato_real.drop(['url','dish_liked','phone'],axis=1) #Dropping the column \"dish_liked\", \"phone\", \"url\" and saving the new dataset as \"zomato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Duplicates\n",
    "zomato.duplicated().sum()\n",
    "zomato.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the NaN values from the dataset\n",
    "zomato.isnull().sum()\n",
    "zomato.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the column names\n",
    "zomato = zomato.rename(columns={'approx_cost(for two people)':'cost','listed_in(type)':'type', 'listed_in(city)':'city'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Transformations\n",
    "zomato['cost'] = zomato['cost'].astype(str) #Changing the cost to string\n",
    "zomato['cost'] = zomato['cost'].apply(lambda x: x.replace(',','.')) #Using lambda function to replace ',' from cost\n",
    "zomato['cost'] = zomato['cost'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing '/5' from Rates\n",
    "zomato = zomato.loc[zomato.rate !='NEW']\n",
    "zomato = zomato.loc[zomato.rate !='-'].reset_index(drop=True)\n",
    "remove_slash = lambda x: x.replace('/5', '') if type(x) == np.str else x\n",
    "zomato.rate = zomato.rate.apply(remove_slash).str.strip().astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the column names\n",
    "zomato.name = zomato.name.apply(lambda x:x.title())\n",
    "zomato.online_order.replace(('Yes','No'),(True, False),inplace=True)\n",
    "zomato.book_table.replace(('Yes','No'),(True, False),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing Mean Rating\n",
    "restaurants = list(zomato['name'].unique())\n",
    "zomato['Mean Rating'] = 0\n",
    "\n",
    "for i in range(len(restaurants)):\n",
    "    zomato['Mean Rating'][zomato['name'] == restaurants[i]] = zomato['rate'][zomato['name'] == restaurants[i]].mean()\n",
    "    \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (1,5))\n",
    "zomato[['Mean Rating']] = scaler.fit_transform(zomato[['Mean Rating']]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the next step is to perform some text preprocessing steps which include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lower Casing\n",
    "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removal of Puctuations\n",
    "import string\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "## Removal of Stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>cuisines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33169</th>\n",
       "      <td>rated 10 ratedn satisfied everything food drin...</td>\n",
       "      <td>North Indian, Chinese, Andhra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18490</th>\n",
       "      <td>rated 40 ratedn small place donÃ£Âƒx83Ã£Â‚x83Ã£Âƒx82...</td>\n",
       "      <td>North Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29256</th>\n",
       "      <td>rated 10 ratedn went today lunch horrible expe...</td>\n",
       "      <td>North Indian, Bengali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30993</th>\n",
       "      <td>rated 10 ratedn ordered spaghetti aglio pasta ...</td>\n",
       "      <td>Continental, Burger, Cafe, Steak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15601</th>\n",
       "      <td>rated 10 ratedn food prepared dirty oil firstl...</td>\n",
       "      <td>Street Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            reviews_list  \\\n",
       "33169  rated 10 ratedn satisfied everything food drin...   \n",
       "18490  rated 40 ratedn small place donÃ£Âƒx83Ã£Â‚x83Ã£Âƒx82...   \n",
       "29256  rated 10 ratedn went today lunch horrible expe...   \n",
       "30993  rated 10 ratedn ordered spaghetti aglio pasta ...   \n",
       "15601  rated 10 ratedn food prepared dirty oil firstl...   \n",
       "\n",
       "                               cuisines  \n",
       "33169     North Indian, Chinese, Andhra  \n",
       "18490                      North Indian  \n",
       "29256             North Indian, Bengali  \n",
       "30993  Continental, Burger, Cafe, Steak  \n",
       "15601                       Street Food  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Removal of URLS\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_urls(text))\n",
    "zomato[['reviews_list', 'cuisines']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTAURANT NAMES:\n",
    "restaurant_names = list(zomato['name'].unique())\n",
    "def get_top_words(column, top_nu_of_words, nu_of_word):\n",
    "    vec = CountVectorizer(ngram_range= nu_of_word, stop_words='english')\n",
    "    bag_of_words = vec.fit_transform(column)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_nu_of_words]\n",
    "    \n",
    "zomato=zomato.drop(['address','rest_type', 'type', 'menu_item', 'votes'],axis=1)\n",
    "import pandas\n",
    "\n",
    "# Randomly sample 60% of your dataframe\n",
    "df_percent = zomato.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) vectors for each document. This will give you a matrix where each column represents a word in the general vocabulary (all words that appear in at least one document) and each column represents a restaurant, as before.\n",
    "\n",
    "TF-IDF is the statistical method of assessing the meaning of a word in a given document. Now, I will use the TF-IDF vectorization on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_percent.set_index('name', inplace=True)\n",
    "indices = pd.Series(df_percent.index)\n",
    "\n",
    "# Creating tf-idf matrix\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df_percent['reviews_list'])\n",
    "\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the last step for creating a Restaurant Recommendation System is to write a function that will recommend restaurants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(name, cosine_similarities = cosine_similarities):\n",
    "    \n",
    "    # Create a list to put top restaurants\n",
    "    recommend_restaurant = []\n",
    "    \n",
    "    # Find the index of the hotel entered\n",
    "    idx = indices[indices == name].index[0]\n",
    "    \n",
    "    # Find the restaurants with a similar cosine-sim value and order them from bigges number\n",
    "    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending=False)\n",
    "    \n",
    "    # Extract top 30 restaurant indexes with a similar cosine-sim value\n",
    "    top30_indexes = list(score_series.iloc[0:31].index)\n",
    "    \n",
    "    # Names of the top 30 restaurants\n",
    "    for each in top30_indexes:\n",
    "        recommend_restaurant.append(list(df_percent.index)[each])\n",
    "    \n",
    "    # Creating the new data set to show similar restaurants\n",
    "    df_new = pd.DataFrame(columns=['cuisines', 'Mean Rating', 'cost'])\n",
    "    \n",
    "    # Create the top 30 similar restaurants with some of their columns\n",
    "    for each in recommend_restaurant:\n",
    "        df_new = df_new.append(pd.DataFrame(df_percent[['cuisines','Mean Rating', 'cost']][df_percent.index == each].sample()))\n",
    "    \n",
    "    # Drop the same named restaurants and sort only the top 10 by the highest rating\n",
    "    df_new = df_new.drop_duplicates(subset=['cuisines','Mean Rating', 'cost'], keep=False)\n",
    "    df_new = df_new.sort_values(by='Mean Rating', ascending=False).head(10)\n",
    "    \n",
    "    print('TOP %s RESTAURANTS LIKE %s WITH SIMILAR REVIEWS: ' % (str(len(df_new)), name))\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 RESTAURANTS LIKE Pai Vihar WITH SIMILAR REVIEWS: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisines</th>\n",
       "      <th>Mean Rating</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Burma Burma</th>\n",
       "      <td>Asian, Burmese</td>\n",
       "      <td>4.74</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cravy Wings</th>\n",
       "      <td>American, Burger, Fast Food</td>\n",
       "      <td>4.11</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ilyazsab The House Of Chicken</th>\n",
       "      <td>Rolls, Kebab</td>\n",
       "      <td>3.84</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andhra Ruchulu</th>\n",
       "      <td>Andhra, North Indian</td>\n",
       "      <td>3.72</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andhra Ruchulu</th>\n",
       "      <td>Andhra, Biryani, North Indian, Chinese</td>\n",
       "      <td>3.72</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dum Biryani Hub</th>\n",
       "      <td>Biryani</td>\n",
       "      <td>3.71</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cafe Aladdin</th>\n",
       "      <td>Cafe, Chinese</td>\n",
       "      <td>3.71</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wow Paratha</th>\n",
       "      <td>North Indian</td>\n",
       "      <td>3.71</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992 Chats - Space</th>\n",
       "      <td>Street Food</td>\n",
       "      <td>3.45</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nys Kitchen</th>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>3.39</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             cuisines  \\\n",
       "Burma Burma                                            Asian, Burmese   \n",
       "Cravy Wings                               American, Burger, Fast Food   \n",
       "Ilyazsab The House Of Chicken                            Rolls, Kebab   \n",
       "Andhra Ruchulu                                   Andhra, North Indian   \n",
       "Andhra Ruchulu                 Andhra, Biryani, North Indian, Chinese   \n",
       "Dum Biryani Hub                                               Biryani   \n",
       "Cafe Aladdin                                            Cafe, Chinese   \n",
       "Wow Paratha                                              North Indian   \n",
       "1992 Chats - Space                                        Street Food   \n",
       "Nys Kitchen                                     North Indian, Chinese   \n",
       "\n",
       "                               Mean Rating   cost  \n",
       "Burma Burma                           4.74    1.5  \n",
       "Cravy Wings                           4.11  400.0  \n",
       "Ilyazsab The House Of Chicken         3.84  250.0  \n",
       "Andhra Ruchulu                        3.72  400.0  \n",
       "Andhra Ruchulu                        3.72    1.0  \n",
       "Dum Biryani Hub                       3.71  700.0  \n",
       "Cafe Aladdin                          3.71  500.0  \n",
       "Wow Paratha                           3.71  400.0  \n",
       "1992 Chats - Space                    3.45  200.0  \n",
       "Nys Kitchen                           3.39  500.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend('Pai Vihar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
